# -*- coding: utf-8 -*-
"""math_of_data_project_phase||.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ReZKx0Lv7IHfwtmN4Eo0Li0c7GWoxJUN
"""

# Cell 1: Libraries & Data Cleaning
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import train_test_split, cross_val_score, learning_curve
from sklearn.preprocessing import StandardScaler, LabelEncoder
# New Libraries based on Rubric
from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 1. Load Data
try:
    df = pd.read_csv('AswanData_weatherdata.csv')
    if 'Unnamed: 0' in df.columns:
        df.drop(columns=['Unnamed: 0'], inplace=True)

    # --- Missing Values Treatment (REQUIRED BY RUBRIC) ---
    print("Checking for missing values...")
    missing_count = df.isnull().sum().sum()
    if missing_count > 0:
        print(f"Found {missing_count} missing values. Filling with median...")
        df.fillna(df.median(numeric_only=True), inplace=True)
    else:
        print("No missing values found. Data is clean.")

    print(f"Data Loaded Successfully. Shape: {df.shape}")
except FileNotFoundError:
    print("Error: File not found.")

# Initialize Scaler and Encoder for later
scaler = StandardScaler()
le = LabelEncoder()

# Cell 2: Advanced Data Analysis (Min, Max, Skewness, Covariance, Tests)

# 1. Feature Engineering (Adding Month)
df['Date'] = pd.to_datetime(df['Date'])
df['Month'] = df['Date'].dt.month

# 2. Binning (Required)
df['Solar_Class'] = pd.qcut(df['Solar(PV)'], q=3, labels=['Low', 'Medium', 'High'])
df['Temp_Class'] = pd.qcut(df['AvgTemperture'], q=3, labels=['Low', 'Medium', 'High'])

# 3. Descriptive Statistics
stats_df = df.describe().transpose()
stats_df['variance'] = df.var(numeric_only=True)
stats_df['skewness'] = df.skew(numeric_only=True)
stats_df['kurtosis'] = df.kurtosis(numeric_only=True)
print("\n--- Descriptive Statistics ---")
print(stats_df[['mean', 'std', 'variance', 'skewness', 'kurtosis']])

# 4. Correlation & Covariance (REQUIRED BY RUBRIC)
print("\n--- Covariance Matrix (Top Features) ---")
cov_matrix = df.cov(numeric_only=True)
print(cov_matrix.iloc[:3, :3]) # Showing a sample of Covariance

# Plotting Correlation Heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap')
plt.show()

# --- إضافة الرسوم البيانية من البروجيكت الأول لضمان بند Data Visualization ---
plt.figure(figsize=(12, 5))

# 1. رسم التوزيع (Histogram)
plt.subplot(1, 2, 1)
sns.histplot(df['Solar(PV)'], kde=True, color='orange')
plt.title('Solar(PV) Distribution')

# 2. رسم الصندوق (Boxplot) لمقارنة الحرارة بالفئات
plt.subplot(1, 2, 2)
sns.boxplot(x='Solar_Class', y='AvgTemperture', data=df, palette='Set2')
plt.title('Temperature Distribution per Solar Class')

plt.tight_layout()
plt.show()

# 5. Statistical Tests
print("\n--- Statistical Hypothesis Testing ---")
group_low = df[df['Solar_Class'] == 'Low']['AvgTemperture']
group_med = df[df['Solar_Class'] == 'Medium']['AvgTemperture']
group_high = df[df['Solar_Class'] == 'High']['AvgTemperture']

# T-test & ANOVA
_, p_val_t = stats.ttest_ind(group_low, group_high)
_, p_val_anova = stats.f_oneway(group_low, group_med, group_high)
# Chi-Square
contingency = pd.crosstab(df['Temp_Class'], df['Solar_Class'])
_, p_val_chi, _, _ = stats.chi2_contingency(contingency)

print(f"- T-test P-value: {p_val_t:.4e}")
print(f"- ANOVA P-value: {p_val_anova:.4e}")
print(f"- Chi-Square P-value: {p_val_chi:.4e}")

# Cell 3: Feature Reduction (PCA, LDA, SVD, Kernel PCA)
print("\n--- 1. Advanced Feature Reduction ---")

# 1. Prepare Data
features_new = ['AvgTemperture', 'AverageDew(point via humidity)', 'Humidity', 'Wind', 'Pressure', 'Month']
X_new = df[features_new]
X_scaled_new = scaler.fit_transform(X_new)
y_encoded = le.fit_transform(df['Solar_Class'])

# A. PCA (Standard)
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled_new)

# B. LDA (Supervised)
lda = LDA(n_components=2)
X_lda = lda.fit_transform(X_scaled_new, y_encoded)

# C. Truncated SVD (REQUIRED BY RUBRIC)
# عبارة عن بيخلي مصفوفة البيانات ل مصفوفات اصغر
# مكون من تلت مصفوفات اول واحدة ديه فيها العينات التانية ديه فيها اهمية كل فيتشر
# التالتة ديه فيها العلاقات بين الاعمدة زي الحرارة الرطوبة كده
# ف بنحذف اللي ملهوش لازمة
svd = TruncatedSVD(n_components=2)
X_svd = svd.fit_transform(X_scaled_new)

# D. Kernel PCA (REQUIRED BY RUBRIC for non-linear)
kpca = KernelPCA(n_components=2, kernel='rbf') # Using RBF kernel for non-linearity
X_kpca = kpca.fit_transform(X_scaled_new)

# حساب وطباعة نسبة التباين (المعلومات) لكل طريقة
print(f"PCA Explained Variance Ratio: {pca.explained_variance_ratio_.sum():.4f}")
print(f"LDA Explained Variance Ratio: {lda.explained_variance_ratio_.sum():.4f}")

# السطر الجديد الخاص بالـ SVD
print(f"SVD Explained Variance Ratio: {svd.explained_variance_ratio_.sum():.4f}")

# Visualizing all 4 methods
plt.figure(figsize=(16, 10))
methods = [('PCA', X_pca), ('LDA', X_lda), ('SVD', X_svd), ('Kernel PCA', X_kpca)]

for i, (name, data) in enumerate(methods):
    plt.subplot(2, 2, i+1)
    sns.scatterplot(x=data[:, 0], y=data[:, 1], hue=df['Solar_Class'], palette='viridis')
    plt.title(f'{name} Projection')
plt.tight_layout()
plt.show()

# Cell 4: Model Implementations (Classification + Linear Regression)
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

print("\n--- 2. Models & K-Fold Evaluation ---")

# 1. تجهيز البيانات (تقسيم 80/20 مطلوب في الروبريك)
# أ. بيانات التصنيف (Classes)
X_train, X_test, y_train, y_test = train_test_split(X_scaled_new, y_encoded, test_size=0.2, random_state=42)
# ب. بيانات الانحدار (الرقم الفعلي للطاقة Solar(PV))
y_reg = df['Solar(PV)']
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_scaled_new, y_reg, test_size=0.2, random_state=42)

# 2. تعريف موديلات التصنيف (مطلب الروبريك)
all_clf_models = {
    'Naive Bayes': GaussianNB(),
    'Decision Tree (Entropy)': DecisionTreeClassifier(criterion='entropy', max_depth=8, random_state=42),
    'LDA Classifier': LDA(),
    'K-NN (Manhattan)': KNeighborsClassifier(n_neighbors=11, metric='manhattan'),
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Neural Network (Deep MLP)': MLPClassifier(hidden_layer_sizes=(64, 32, 16), max_iter=2000, random_state=42)
}

# 3. حساب K-Fold للتصنيف (مطلب إجباري)
print(f"{'Classification Model':<25} | {'CV Avg Accuracy':<15}")
print("-" * 45)
for name, model in all_clf_models.items():
    cv_scores = cross_val_score(model, X_scaled_new, y_encoded, cv=5)
    print(f"{name:<25} | {cv_scores.mean():.4f}")

# 4. تنفيذ وتقييم Linear Regression (مطلب إجباري منفصل في الروبريك)
print("\n--- Linear Regression Model (Evaluation Metrics) ---")
lin_reg = LinearRegression()
lin_reg.fit(X_train_reg, y_train_reg)
y_pred_reg = lin_reg.predict(X_test_reg)

# حساب المقاييس المطلوبة في الروبريك (MAE, RMSE, R2)
mae = mean_absolute_error(y_test_reg, y_pred_reg)
rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred_reg))
r2 = r2_score(y_test_reg, y_pred_reg)

print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-Squared (R2 Score): {r2:.4f}")

# 5. الجدول النهائي لموديلات التصنيف (لمسة البروجيكت الأول)
final_comparison = []
for name, model in all_clf_models.items():
    model.fit(X_train, y_train)
    acc = accuracy_score(y_test, model.predict(X_test))
    final_comparison.append({'Model': name, 'Accuracy': f"{acc:.4f}", 'Error Rate': f"{1-acc:.4f}"})

print("\n--- Final Summary Table (Classification) ---")
print(pd.DataFrame(final_comparison))

print("\n*Note: Naive Bayes and Linear Regression fulfill distinct rubric requirements for Classification and Regression tasks.")

# Cell 5: Comprehensive Evaluation (Neural Network - Phase II Requirements)
print("\n--- 3. Final Model Evaluation & Overfitting Analysis ---")

from sklearn.model_selection import learning_curve
from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report, accuracy_score
from sklearn.preprocessing import label_binarize

# 1. اختيار الموديل الأفضل (تم تعديل الاسم من all_models إلى all_clf_models)
try:
    final_model_name = 'Neural Network (Deep MLP)'
    # هنا التصحيح: استدعاء الاسم الصحيح من الخلية رقم 4
    final_model = all_clf_models[final_model_name]
except (NameError, KeyError):
    print("Error: Make sure Cell 4 was run and 'all_clf_models' is defined!")

# 2. حساب الدقة ومعدل الخطأ (Accuracy & Error Rate)
y_pred = final_model.predict(X_test)
acc = accuracy_score(y_test, y_pred)
error_rate = 1 - acc
print(f"Final Model Selected: {final_model_name}")
print(f"Test Accuracy: {acc:.4f} | Test Error Rate: {error_rate:.4f}")

# 3. مصفوفة الارتباك (Confusion Matrix)
print("\n--- Interpreting Confusion Matrix ---")
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(7, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='PuBu',
            xticklabels=le.classes_, yticklabels=le.classes_)
plt.title(f'Confusion Matrix - {final_model_name}')
plt.ylabel('Actual Classes')
plt.xlabel('Predicted Classes')
plt.show()

# 4. تحليل الـ Overfitting والـ Underfitting (Learning Curves)
# مطلب أساسي في الروبريك لتوضيح استقرار الموديل
print("\n--- Generating Overfitting/Underfitting Curves ---")
train_sizes, train_scores, test_scores = learning_curve(
    final_model, X_scaled_new, y_encoded, cv=5, scoring='accuracy', n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 10)
)



plt.figure(figsize=(10, 6))
plt.plot(train_sizes, np.mean(train_scores, axis=1), label='Training Accuracy', color='blue', marker='o')
plt.plot(train_sizes, np.mean(test_scores, axis=1), label='Cross-Validation Accuracy', color='red', marker='s')
plt.title('Learning Curves (Stability & Overfitting Analysis)')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend(loc='best')
plt.grid(True)
plt.show()

# 5. التقرير التفصيلي (Precision, Recall, F-measure)
print("\n--- Detailed Classification Report ---")
print(classification_report(y_test, y_pred, target_names=le.classes_))

# 6. رسم الـ ROC Curve لكل الفئات (Multi-class ROC)
y_test_bin = label_binarize(y_test, classes=[0, 1, 2])
y_score = final_model.predict_proba(X_test)

plt.figure(figsize=(10, 6))
for i, label in enumerate(le.classes_):
    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])
    plt.plot(fpr, tpr, lw=2, label=f'ROC {label} (AUC = {auc(fpr, tpr):.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.title('Multi-class ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

# 7. جدول المقارنة النهائي (Final Comparisons Table)
# تم تعديل الاسم هنا أيضاً ليعمل بشكل صحيح
final_results = []
for name, model in all_clf_models.items():
    score = model.score(X_test, y_test)
    final_results.append({'Model': name, 'Accuracy': f"{score:.4f}", 'Error Rate': f"{1-score:.4f}"})

print("\n--- Final Comparisons Summary (Classification) ---")
print(pd.DataFrame(final_results))